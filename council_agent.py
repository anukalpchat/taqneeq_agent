"""
SENTINEL Council Agent - The Strategist
Multi-persona LLM reasoning engine for payment failure pattern detection

Architecture: Single LLM call with three embedded perspectives:
- CFO (Conservative Finance): Profit maximization, cost control
- CTO (Technical Operations): Infrastructure patterns, outage prediction  
- Risk Manager (Security): Fraud detection, compliance enforcement
"""

import os
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

import pandas as pd
from dotenv import load_dotenv
from groq import Groq

from models import Transaction, FailureCluster, AgentDecision, PerformanceMetrics
from config import (
    REROUTE_COST,
    MARGIN_RATE,
    MIN_PATTERN_SIZE,
    SPIKE_MULTIPLIER,
    LLM_MODEL,
    LLM_TEMPERATURE,
    MAX_OUTPUT_TOKENS
)

# Load environment variables
load_dotenv()

# Configure Groq API
api_key = os.getenv("GROQ_API_KEY")
if not api_key:
    raise ValueError("Missing API key. Set GROQ_API_KEY in .env file")
client = Groq(api_key=api_key)

# Pattern history tracking
PATTERN_HISTORY_PATH = "data/pattern_history.json"


class CouncilAgent:
    """
    The Strategist - Multi-persona LLM for pattern detection and decision recommendation
    """
    
    def __init__(self, model_name: str = LLM_MODEL):
        """Initialize Council Agent with Groq Llama model"""
        self.model_name = model_name
        self.client = client
        self.decisions: List[AgentDecision] = []
        self.metrics: Optional[PerformanceMetrics] = None
        self.pattern_history = self._load_pattern_history()
        self.calibrations: List[Dict[str, Any]] = []
        
    def analyze_failures(self, transactions_path: str = "data/transactions.json") -> Dict[str, Any]:
        """
        Full pipeline: Load ‚Üí Filter ‚Üí Cluster ‚Üí Analyze ‚Üí Validate ‚Üí Save
        
        Returns:
            dict: {
                "decisions": List[AgentDecision],
                "metrics": PerformanceMetrics,
                "metadata": execution details
            }
        """
        start_time = time.time()
        
        print("üîç Loading transactions from", transactions_path)
        df = self._load_transactions(transactions_path)
        print(f"   ‚úì Loaded {len(df)} transactions")
        
        failures = self._filter_failures(df)
        print(f"   ‚úì Found {len(failures)} failures ({len(failures)/len(df)*100:.1f}% failure rate)")
        
        print("\nüìä Aggregating failure patterns...")
        clusters = self._create_clusters(failures, df)
        print(f"   ‚úì Created {len(clusters)} clusters")
        
        # Rank by impact and select top clusters
        top_clusters = self._rank_clusters(clusters)
        print(f"   ‚úì Ranked by impact, selected top {len(top_clusters)} for analysis")
        
        # Handle edge case: no clusters found
        if len(top_clusters) == 0:
            print("\n‚ö†Ô∏è  No significant patterns found (all clusters below MIN_PATTERN_SIZE).")
            return {
                "decisions": [],
                "metrics": PerformanceMetrics(
                    total_transactions=len(df),
                    total_failures=len(failures),
                    reroutes_executed=0,
                    reroutes_ignored=0,
                    alerts_raised=0,
                    total_cost=0.0,
                    total_revenue_saved=0.0,
                    net_profit=0.0,
                    patterns_discovered=0,
                    decision_accuracy=0.0
                ),
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "model_used": LLM_MODEL,
                    "inference_time_seconds": round(time.time() - start_time, 2),
                    "total_decisions": 0
                }
            }
        
        # Store clusters for accurate profit calculations
        self.analyzed_clusters = top_clusters
        
        print(f"\nüìä Calibrating confidence baselines...")
        calibrations = self._calibrate_confidence(top_clusters)
        print(f"   ‚úì Calibrated {len(calibrations)} patterns")
        
        print(f"\nüß† Analyzing patterns with {LLM_MODEL}...")
        decisions = self._analyze_with_llm(top_clusters, calibrations)
        print(f"   ‚úì LLM call completed ({time.time() - start_time:.1f}s)")
        print(f"   ‚úì Received {len(decisions)} decisions")
        
        print("\n‚úÖ Validating decisions...")
        valid_decisions = self._validate_decisions(decisions)
        print(f"   ‚úì {len(valid_decisions)}/{len(decisions)} decisions passed validation")
        
        print("\nüí∞ Calculating metrics...")
        metrics = self._calculate_metrics(valid_decisions, len(df), len(failures))
        print(f"   Net Profit: ‚Çπ{metrics.net_profit:,.0f}")
        print(f"   Patterns: {metrics.patterns_discovered} discovered")
        print(f"   Decisions: {sum([metrics.reroutes_executed, metrics.reroutes_ignored, metrics.alerts_raised])} total")
        
        print("\nüíæ Saving to data/decisions.json...")
        output = self._save_results(valid_decisions, metrics, start_time)
        print("   ‚úì Complete!")
        
        return output
    
    def _load_transactions(self, path: str) -> pd.DataFrame:
        """Load and validate transaction data"""
        if not os.path.exists(path):
            raise FileNotFoundError(
                f"Transactions file not found: {path}\n"
                f"Run chaos_engine.py first to generate synthetic data."
            )
        
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Convert to DataFrame
        df = pd.DataFrame(data)
        
        # Parse timestamps
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        # Validate schema (sample first few rows)
        for row in data[:5]:
            Transaction(**row)  # Pydantic validation
        
        return df
    
    def _filter_failures(self, df: pd.DataFrame) -> pd.DataFrame:
        """Extract failed transactions only"""
        return df[df['status'] == 'FAILED'].copy()
    
    def _create_clusters(self, failures: pd.DataFrame, all_df: pd.DataFrame) -> List[FailureCluster]:
        """
        Group failures into patterns for LLM analysis
        
        Clustering strategy:
        - Primary: (bank, card_type)
        - Secondary: amount_range, time_window, merchant_category, customer_tier
        """
        clusters = []
        
        # Add hour column for temporal grouping
        failures['hour'] = failures['timestamp'].dt.hour  # type: ignore
        
        # Define amount buckets
        def get_amount_bucket(amount):
            if amount < 100:
                return "<100"
            elif amount < 1000:
                return "100-1000"
            elif amount < 5000:
                return "1000-5000"
            else:
                return ">5000"
        
        failures['amount_bucket'] = failures['amount'].apply(get_amount_bucket)
        
        # Group by multiple dimensions
        grouping_keys = ['bank', 'card_type', 'amount_bucket']
        
        # Sort to ensure deterministic ordering
        for group_key, group_df in failures.groupby(grouping_keys, sort=True):
            if len(group_df) < MIN_PATTERN_SIZE:
                continue  # Skip small clusters
            
            bank, card_type, amount_range = group_key
            
            # Calculate failure rate for this segment
            segment_total = len(all_df[
                (all_df['bank'] == bank) & 
                (all_df['card_type'] == card_type) &
                (all_df['amount'].apply(get_amount_bucket) == amount_range)
            ])
            
            failure_rate = len(group_df) / segment_total if segment_total > 0 else 0
            
            # Time window (most common hours)
            hour_counts = group_df['hour'].value_counts()
            if len(hour_counts) > 0:
                dominant_hours = hour_counts.head(2).index.tolist()
                time_window = f"{min(dominant_hours):02d}:00-{max(dominant_hours)+1:02d}:00"
            else:
                time_window = "all_day"
            
            # Error codes (top 3)
            error_codes = group_df['error_code'].value_counts().head(3).index.tolist()
            
            # Create cluster
            cluster = FailureCluster(
                bank=bank,
                card_type=card_type,
                amount_range=amount_range,
                count=len(group_df),
                avg_amount=float(group_df['amount'].mean()),
                failure_rate=failure_rate,
                time_window=time_window,
                error_codes=error_codes
            )
            
            clusters.append(cluster)
        
        return clusters
    
    def _rank_clusters(self, clusters: List[FailureCluster], max_clusters: int = 12) -> List[FailureCluster]:
        """
        Rank clusters by business impact (count √ó avg_amount)
        Keep top N for LLM analysis
        """
        # Calculate impact score with stable secondary keys for deterministic ordering
        ranked = sorted(
            clusters,
            key=lambda c: (
                -(c.count * c.avg_amount),  # Primary: impact (descending)
                c.bank,                      # Secondary: alphabetical
                c.card_type,                 # Tertiary: alphabetical
                c.amount_range               # Quaternary: alphabetical
            )
        )
        
        return ranked[:max_clusters]
    
    def _build_system_prompt(self, confidence_briefing: Optional[str] = None) -> str:
        """
        Construct the multi-persona system prompt
        """
        confidence_context = ""
        if confidence_briefing:
            confidence_context = f"\n\n## CONFIDENCE CALIBRATION BRIEFING\n\n{confidence_briefing}\n\nThis briefing reflects the system's historical performance on similar patterns. Factor this into your confidence scoring."
        
        return f"""You are the SENTINEL Council - a payment operations decision system with two expert advisors and one moderator.{confidence_context}

## COUNCIL STRUCTURE

**CFO (Chief Financial Officer)**
Focus: Profit maximization, capital efficiency
Formula: Net_Benefit = (avg_amount √ó 0.02 √ó count) - (‚Çπ15 √ó count)
Break-even: ‚Çπ750 transaction (‚Çπ750 √ó 0.02 = ‚Çπ15 reroute cost)
Bias: Conservative on spending, rejects unprofitable fixes

**CTO (Chief Technology Officer)**
Focus: System reliability, customer experience, SLA (99.5% target)
Priorities: VIP customers (>‚Çπ3K), infrastructure health, failure trends
Bias: Liberal on reliability spending, rejects low-impact issues

**Moderator (Chief Decision Officer)**
Role: Synthesize CFO and CTO perspectives into final decision
Responsibility: Break ties, balance profit vs reliability, explain trade-offs
Has NO personal bias - purely facilitative

## DECISION FRAMEWORK

### PHASE 1: INDIVIDUAL ANALYSIS

**CFO Calculates:**
```
Potential_Revenue = avg_amount √ó 0.02 √ó count
Intervention_Cost = ‚Çπ15 √ó count
Net_Benefit = Potential_Revenue - Intervention_Cost
```

**CFO Decision Rules:**
- Net_Benefit > ‚Çπ500 ‚Üí Recommend REROUTE
- Net_Benefit ‚Çπ0 to ‚Çπ500 ‚Üí Marginal (defer to CTO/Moderator)
- Net_Benefit < 0 ‚Üí Recommend IGNORE

**CTO Assesses:**

*Temporal Pattern:*
- **STABLE** (failure rate consistent 1+ hrs) ‚Üí Systemic issue, reroutable
- **SPIKE** (failure rate 3x in <30min) ‚Üí Infrastructure failing, don't reroute
- **DECLINING** (rate decreasing) ‚Üí Self-healing, ignore

*Customer Impact:*
- **CRITICAL**: VIP customers (>‚Çπ3K), >80% failure rate
- **HIGH**: Regular customers, 50-80% failure rate  
- **LOW**: Micro-transactions (<‚Çπ200), <50% failure rate

**CTO Decision Rules:**
- SPIKE detected ‚Üí Recommend ALERT (infrastructure emergency)
- CRITICAL impact + STABLE ‚Üí Recommend REROUTE (SLA violation)
- LOW impact + any pattern ‚Üí Recommend IGNORE (low priority)

### PHASE 2: MODERATOR SYNTHESIS

**When CFO & CTO Agree:**
- Both say REROUTE ‚Üí Final: REROUTE (unanimous, confidence 0.9+)
- Both say IGNORE ‚Üí Final: IGNORE (unanimous, confidence 0.9+)
- Both say ALERT ‚Üí Final: ALERT (unanimous, confidence 0.85+)

**When CFO & CTO Disagree:**

Apply Tiebreaker Hierarchy:

1. **Infrastructure Emergency Overrides Profit**
   - CTO says ALERT (spike detected) + CFO says REROUTE (profitable)
   - Moderator sides with CTO ‚Üí ALERT
   - Reason: Backup gateway will fail too; waste of ‚Çπ15/txn
   - Example: ICICI 5%‚Üí18% spike

2. **Severe Loss Overrides Customer Impact**
   - CFO says IGNORE (Net < -‚Çπ1,000) + CTO says REROUTE (customer impact)
   - Moderator sides with CFO ‚Üí IGNORE
   - Reason: Platform cannot sustain massive losses
   - Example: SBI <‚Çπ100 with -‚Çπ1,797 net loss

3. **VIP Retention Breaks Marginal Profit Ties**
   - CFO says marginal/negative (‚Çπ0-‚Çπ500) + CTO says VIP CRITICAL
   - Moderator sides with CTO ‚Üí REROUTE
   - Reason: Customer lifetime value > immediate ROI
   - Example: VIP Travel ‚Çπ330 profit but high retention value

4. **Low Confidence Degrades to Safer Action**
   - Pattern ambiguous or insufficient data
   - Moderator chooses lower-risk option
   - REROUTE (risky spend) ‚Üí ALERT (investigate first)
   - IGNORE (risky reputation) ‚Üí REROUTE (test with small spend)

## OUTPUT FORMAT

Return JSON array with this EXACT structure for EACH pattern:

```json
{{
  "pattern_detected": "Specific: bank + card_type + amount_range + time_window",
  "affected_volume": 47,
  "avg_amount": 7842.50,
  "cost_analysis": "CFO: Revenue ‚Çπ7,332 (47√ó‚Çπ156) - Cost ‚Çπ705 = Net +‚Çπ6,627. CTO: VIP customers, SLA violation.",
  "temporal_signal": "stable",
  "risk_category": "payment_failure",
  "decision": "REROUTE",
  "reasoning": "CFO perspective: Excellent ROI with ‚Çπ6,627 net benefit‚Äîclearly profitable intervention. CTO perspective: 98% failure rate affecting VIP customers for 2 hours is critical SLA violation. Pattern stability indicates systemic HDFC issue. Moderator synthesis: Both perspectives unanimously agree on REROUTE. Financial logic and operational logic perfectly aligned.",
  "confidence": 0.95
}}
```

CRITICAL: Use this EXACT schema. Do NOT add extra nested objects.

## ACTIONS AVAILABLE

**REROUTE**: Pay ‚Çπ15/txn to retry via backup gateway
- When: Net_Benefit > 0 AND (stable pattern OR VIP impact)

**IGNORE**: Let transactions fail, preserve capital
- When: Net_Benefit < 0 OR low customer impact

**ALERT**: Notify ops team, don't reroute
- When: Infrastructure spike (3x failure rate in <30min)

## CRITICAL RULES

1. **Show all three perspectives** in every decision
2. **Label agreement_type** clearly:
   - "unanimous" - Both CFO and CTO agree
   - "cfo_priority" - Moderator sided with CFO
   - "cto_priority" - Moderator sided with CTO
   - "moderator_tiebreak" - Close call, judgment used

3. **No contradictions**:
   - If final_action = REROUTE, then net_benefit must be ‚â• 0
   - If final_action = IGNORE, then either net_benefit < 0 OR customer_impact = LOW
   - If final_action = ALERT, then temporal_signal must be "spike_detected"

4. **Confidence scoring**:
   - 0.9-1.0: Unanimous, clear case
   - 0.75-0.89: Strong majority, minor concern
   - 0.6-0.74: Tiebreak used, marginal case
   - <0.6: Insufficient data, default to safer option

5. **Cost analysis must show math** with ‚Çπ symbol

## PATTERN CLASSIFICATION EXAMPLES

### Pattern Type A: Unanimous REROUTE
Characteristics: High net benefit + Critical customer impact + Stable

**Input**: HDFC Rewards >‚Çπ5K, 47 txn, avg ‚Çπ7,842, 98% failure, stable, VIP
**Output**:
```json
{{
  "pattern_detected": "HDFC Rewards >‚Çπ5K failing 14:00-16:00",
  "affected_volume": 47,
  "avg_amount": 7842.50,
  "cost_analysis": "Reroute cost: ‚Çπ705 (47√ó‚Çπ15). Revenue saved: ‚Çπ7,332 (47√ó‚Çπ156.85 avg margin). Net: +‚Çπ6,627",
  "temporal_signal": "stable",
  "risk_category": "payment_failure",
  "decision": "REROUTE",
  "reasoning": "CFO perspective: Excellent ROI with ‚Çπ156.85 average margin per transaction. Total net benefit of ‚Çπ6,627 is substantial, not marginal. CTO perspective: 98% failure rate affecting VIP customers for 2 hours violates our 99.5% SLA commitment. Pattern stability indicates systemic HDFC Rewards issue, making reroute to Razorpay likely to succeed. Moderator synthesis: Unanimous decision‚Äîboth financial profitability and operational reliability perspectives strongly align. Zero conflict between profit and customer impact logic.",
  "confidence": 0.95
}}
```

### Pattern Type B: Unanimous IGNORE
Characteristics: Negative net benefit + Low customer impact

**Input**: SBI <‚Çπ100, 127 txn, avg ‚Çπ42, 76% failure, stable
**Output**:
```json
{{
  "pattern_detected": "SBI micro-transactions <‚Çπ100",
  "affected_volume": 127,
  "avg_amount": 42.0,
  "cost_analysis": "Reroute cost: ‚Çπ1,905 (127√ó‚Çπ15). Revenue saved: ‚Çπ108 (127√ó‚Çπ0.85 avg margin). Net: -‚Çπ1,797",
  "temporal_signal": "stable",
  "risk_category": "payment_failure",
  "decision": "IGNORE",
  "reasoning": "CFO perspective: Severe negative ROI‚Äîwe'd pay ‚Çπ15 to save ‚Çπ0.85 per transaction, creating ‚Çπ14.15 loss per transaction. Total net loss of ‚Çπ1,797 is financially indefensible. CTO perspective: Micro-transaction segment has high customer retry tolerance. Users successfully switch to alternate payment methods within minutes. Platform SLA impact minimal (<2% of total volume). Moderator synthesis: Perfect alignment between financial and operational logic. Both perspectives strongly recommend IGNORE. Resources better deployed on high-value patterns like HDFC Rewards.",
  "confidence": 1.0
}}
```

### Pattern Type C: CTO Override (Infrastructure Alert)
Characteristics: Spiking failure rate (infrastructure emergency)

**Input**: ICICI Debit, 59 txn, failure 5%‚Üí18% in 10min
**Output**:
```json
{{
  "pattern_detected": "ICICI Debit infrastructure spike 5%‚Üí18% in 10min",
  "affected_volume": 59,
  "avg_amount": 2200.0,
  "cost_analysis": "Reroute would cost: ‚Çπ885 (59√ó‚Çπ15). Revenue potential: ‚Çπ2,596. However, spike indicates imminent total outage.",
  "temporal_signal": "spike_detected",
  "risk_category": "server_outage",
  "decision": "ALERT",
  "reasoning": "CFO perspective: Calculates ‚Çπ1,711 net profit opportunity (‚Çπ2,596 revenue - ‚Çπ885 cost). Appears profitable on paper. CTO perspective: Failure rate tripled in 10 minutes‚Äîclassic canary warning of infrastructure degradation. Predicts Phase 2 total ICICI outage within 30-60 minutes. Moderator decision: Applying Tiebreaker Rule #1 (Infrastructure Emergency Overrides Profit). Siding with CTO. Spending ‚Çπ885 to reroute these 59 transactions would be wasted money when ICICI's backend infrastructure collapses completely. ALERT ops team to prepare system-wide failover instead of treating symptoms with transactional reroutes.",
  "confidence": 0.82
}}
```

### Pattern Type D: Moderator Tiebreak (VIP Retention)
Characteristics: Marginal profit + VIP segment

**Input**: VIP Travel, 22 txn, avg ‚Çπ3,500, 65% failure, weekend
**Output**:
```json
{{
  "pattern_detected": "VIP Travel bookings failing on weekends",
  "affected_volume": 22,
  "avg_amount": 3500.0,
  "cost_analysis": "Reroute cost: ‚Çπ330 (22√ó‚Çπ15). Revenue saved: ‚Çπ1,540 (22√ó‚Çπ70 avg margin). Net: +‚Çπ1,210",
  "temporal_signal": "stable",
  "risk_category": "payment_failure",
  "decision": "REROUTE",
  "reasoning": "CFO perspective: Marginal immediate profit of ‚Çπ1,210. Below the ‚Çπ1,500 threshold for strong conviction. Notes opportunity cost of deploying ‚Çπ330 elsewhere. CTO perspective: VIP customers making time-sensitive Travel purchases. 65% failure rate creates poor user experience. Customer lifetime value (avg ‚Çπ15K annually) far exceeds immediate transaction margin. Retention critical for long-term revenue. Moderator decision: Applying Tiebreaker Rule #3 (VIP Retention Breaks Marginal Profit Ties). While immediate ROI is small, losing 22 VIP Travel customers to competitors represents ‚Çπ330K annual revenue at risk. Small investment of ‚Çπ330 justified for strategic retention value. REROUTE with medium confidence.",
  "confidence": 0.72
}}
```

### Pattern Type E: Unanimous ALERT (System-Wide Issue)
Characteristics: All providers affected (platform infrastructure problem)

**Input**: All banks 2x latency spike, 156 txn affected
**Output**:
```json
{{
  "pattern_detected": "System-wide latency spike across all providers",
  "affected_volume": 156,
  "avg_amount": 2000.0,
  "cost_analysis": "Reroute would cost: ‚Çπ2,340 (156√ó‚Çπ15). Revenue potential: ‚Çπ6,240. However, all providers stressed equally.",
  "temporal_signal": "spike_detected",
  "risk_category": "high_traffic",
  "decision": "ALERT",
  "reasoning": "CFO perspective: Calculates ‚Çπ3,900 net profit on paper (‚Çπ6,240 revenue - ‚Çπ2,340 cost). However, recognizes that if ALL providers are stressed, backup gateways are equally degraded. Rerouting would waste ‚Çπ2,340. CTO perspective: Latency doubled across ALL providers simultaneously. This is not a provider-specific issue‚Äîindicates our own platform infrastructure under stress (database connection saturation, load balancer capacity exhaustion). Requires system-level intervention, not transaction-level rerouting. Moderator synthesis: Unanimous ALERT. Both perspectives agree this needs ops team to scale infrastructure, throttle traffic, or implement circuit breaking. Rerouting individual transactions won't solve the underlying platform capacity problem.",
  "confidence": 0.88
}}
```

## TEMPORAL SIGNAL DEFINITIONS

**stable**: Failure rate consistent for 1+ hours
- Example: "HDFC 98% failure for 2 hours"
- Indicates: Systemic provider issue, rerouting likely succeeds

**spike_detected**: Failure rate increased 3x in <30 minutes
- Example: "ICICI 5%‚Üí18% in 10 minutes"
- Indicates: Infrastructure degradation, impending total outage

**declining**: Failure rate decreasing over time
- Example: "Failure rate 80%‚Üí60%‚Üí40% over 1 hour"
- Indicates: Self-healing, transient issue resolving

## CUSTOMER IMPACT LEVELS

**CRITICAL**: VIP tier (>‚Çπ3K avg) OR failure rate >80% OR time-sensitive (Travel, E-commerce)
**HIGH**: Regular customers with 50-80% failure rate
**MODERATE**: Regular customers with 20-50% failure rate
**LOW**: Micro-transactions (<‚Çπ200) OR <20% failure rate

## YOUR RESPONSIBILITIES

As the council system, you must:

1. **Generate all three perspectives** for each pattern (CFO analysis, CTO analysis, Moderator synthesis)
2. **Show your work** - include all calculations in CFO analysis
3. **Apply tiebreaker rules consistently** when CFO and CTO disagree
4. **Label agreement clearly** - state if unanimous or which side moderator chose
5. **Ensure alignment** - final_action must match the logic presented
6. **Adjust confidence** based on agreement strength

Now analyze these failure clusters and return your council's structured decisions."""

    def _analyze_with_llm(self, clusters: List[FailureCluster], calibrations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Send clusters to LLM for analysis with confidence calibration
        """
        # Aggregate confidence briefings
        briefings = [cal.get("council_briefing", "") for cal in calibrations if cal.get("council_briefing")]
        confidence_briefing = "\n".join([f"‚Ä¢ {b}" for b in briefings]) if briefings else None
        
        # Build system prompt with calibration context
        system_prompt = self._build_system_prompt(confidence_briefing)
        
        # Build user message (clusters as JSON)
        clusters_json = json.dumps([c.model_dump() for c in clusters], indent=2)
        
        user_message = f"""Analyze these failure clusters and provide decisions:

{clusters_json}

Return a JSON array of decisions following the format specified in the system prompt."""
        
        # Combine into single prompt
        full_prompt = system_prompt + "\n\n" + user_message
        
        # Call Groq with retry logic
        max_retries = 2
        for attempt in range(max_retries + 1):
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {
                            "role": "system",
                            "content": system_prompt
                        },
                        {
                            "role": "user",
                            "content": user_message
                        }
                    ],
                    temperature=0,  # Minimizes randomness (but not completely deterministic)
                    max_tokens=MAX_OUTPUT_TOKENS,
                    response_format={"type": "json_object"}
                    # Note: Even with temperature=0, LLM APIs may have slight non-determinism
                    # due to distributed infrastructure, floating-point ops, and sampling
                )
                
                # Parse JSON response
                response_text = response.choices[0].message.content
                if not response_text:
                    raise ValueError("Empty response from LLM")
                response_text = response_text.strip()
                
                # Handle markdown code blocks if present
                if response_text.startswith("```json"):
                    response_text = response_text.split("```json")[1].split("```")[0].strip()
                elif response_text.startswith("```"):
                    response_text = response_text.split("```")[1].split("```")[0].strip()
                
                # Parse the JSON response
                response_data = json.loads(response_text)
                
                # Handle both wrapped and unwrapped arrays
                if "decisions" in response_data:
                    decisions = response_data["decisions"]
                elif isinstance(response_data, list):
                    decisions = response_data
                elif isinstance(response_data, dict):
                    decisions = [response_data]
                else:
                    decisions = response_data
                
                return decisions
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Attempt {attempt + 1} failed: {str(e)}")
                if attempt == max_retries:
                    print("   ‚ùå All retries exhausted. Returning fallback decision.")
                    # Return safe fallback (will fail validation but won't crash)
                    return [{
                        "pattern_detected": "LLM analysis failed - technical error",
                        "affected_volume": 1,
                        "avg_amount": 0.01,
                        "cost_analysis": "Unable to calculate due to API failure or rate limit exceeded",
                        "temporal_signal": "stable",
                        "risk_category": "payment_failure",
                        "decision": "IGNORE",
                        "reasoning": "Council unable to analyze pattern due to technical error (API failure, rate limit, or network issue). Defaulting to IGNORE to preserve capital and avoid risky decisions without proper analysis.",
                        "confidence": 0.0
                    }]
                time.sleep(2)  # Wait before retry
        
        # Should never reach here due to return in except block
        return []
    
    def _validate_decisions(self, decisions: List[Dict[str, Any]]) -> List[AgentDecision]:
        """
        Validate each decision with Pydantic schema
        """
        valid_decisions = []
        
        for i, decision_dict in enumerate(decisions):
            try:
                decision = AgentDecision(**decision_dict)
                valid_decisions.append(decision)
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Decision {i+1} failed validation: {str(e)[:200]}")
                # Skip invalid decisions
        
        if len(valid_decisions) == 0:
            print("   ‚ùå No valid decisions! All LLM outputs failed validation.")
        
        return valid_decisions
    
    def _calculate_metrics(
        self, 
        decisions: List[AgentDecision],
        total_transactions: int,
        total_failures: int
    ) -> PerformanceMetrics:
        """
        Calculate aggregate performance metrics using actual cluster data
        (not LLM-generated values which may be rounded/modified)
        """
        reroutes = [d for d in decisions if d.decision == "REROUTE"]
        ignores = [d for d in decisions if d.decision == "IGNORE"]
        alerts = [d for d in decisions if d.decision == "ALERT"]
        
        # Match decisions to original clusters for accurate calculations
        total_cost = 0.0
        total_revenue = 0.0
        
        for decision in reroutes:
            # Find matching cluster by pattern similarity
            matched_cluster = self._find_matching_cluster(decision)
            
            if matched_cluster:
                # Use actual cluster data for calculations
                cost = matched_cluster.count * REROUTE_COST
                revenue = matched_cluster.count * matched_cluster.avg_amount * MARGIN_RATE
                total_cost += cost
                total_revenue += revenue
            else:
                # Fallback to LLM values if no cluster match (shouldn't happen)
                total_cost += decision.affected_volume * REROUTE_COST
                total_revenue += decision.affected_volume * decision.avg_amount * MARGIN_RATE
        
        net_profit = total_revenue - total_cost
        
        # Count patterns
        patterns_discovered = len(decisions)
        
        # Decision accuracy (placeholder - would need ground truth)
        decision_accuracy = 0.85  # Assuming 85% for now
        
        return PerformanceMetrics(
            total_transactions=total_transactions,
            total_failures=total_failures,
            reroutes_executed=len(reroutes),
            reroutes_ignored=len(ignores),
            alerts_raised=len(alerts),
            total_cost=total_cost,
            total_revenue_saved=total_revenue,
            net_profit=net_profit,
            patterns_discovered=patterns_discovered,
            decision_accuracy=decision_accuracy
        )
    
    def _find_matching_cluster(self, decision: AgentDecision) -> Optional[FailureCluster]:
        """
        Match a decision back to its original cluster by pattern similarity.
        Uses fuzzy matching on bank, card_type, and amount_range.
        """
        if not hasattr(self, 'analyzed_clusters'):
            return None
        
        pattern_lower = decision.pattern_detected.lower()
        
        for cluster in self.analyzed_clusters:
            # Check if cluster attributes appear in pattern description
            bank_match = cluster.bank.lower() in pattern_lower
            card_match = cluster.card_type.lower() in pattern_lower
            amount_match = cluster.amount_range.lower() in pattern_lower or \
                          cluster.amount_range.replace('<', '').replace('>', '') in pattern_lower
            
            # Match if at least 2 of 3 key attributes are present
            matches = sum([bank_match, card_match, amount_match])
            if matches >= 2:
                return cluster
        
        return None
    
    def _load_pattern_history(self) -> Dict[str, Dict[str, Any]]:
        """Load pattern history from disk or create empty structure"""
        Path("data").mkdir(exist_ok=True)
        
        if os.path.exists(PATTERN_HISTORY_PATH):
            with open(PATTERN_HISTORY_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        return {}
    
    def _save_pattern_history(self):
        """Persist pattern history to disk"""
        with open(PATTERN_HISTORY_PATH, 'w', encoding='utf-8') as f:
            json.dump(self.pattern_history, f, indent=2, ensure_ascii=False)
    
    def _classify_pattern_type(self, cluster: FailureCluster) -> str:
        """Classify cluster into pattern type category"""
        pattern_text = f"{cluster.bank} {cluster.card_type} {cluster.amount_range} {cluster.failure_rate}".lower()
        
        # Spike detection: high failure rate in short time window
        if cluster.failure_rate > 0.7 and "spike" in pattern_text:
            return "bank_spike"
        
        # Micro transactions: very low amounts
        if "<100" in cluster.amount_range or "<200" in cluster.amount_range:
            return "micro_transaction"
        
        # VIP anomaly: high value transactions
        if ">5000" in cluster.amount_range or cluster.avg_amount > 5000:
            return "vip_anomaly"
        
        # Card testing: many small failures
        if cluster.count > 50 and cluster.failure_rate > 0.8:
            return "card_testing"
        
        # High risk: specific error codes or patterns
        if "fraud" in pattern_text or "security" in pattern_text:
            return "high_risk_payment"
        
        # Default: general payment failure
        return "payment_failure"
    
    def _calibrate_confidence(self, clusters: List[FailureCluster]) -> List[Dict[str, Any]]:
        """
        Run confidence calibration for each cluster based on historical performance
        """
        calibrations = []
        current_time = datetime.now()
        
        for idx, cluster in enumerate(clusters):
            pattern_id = f"PAT-{cluster.bank}-{cluster.card_type}-{idx:04d}"
            pattern_type = self._classify_pattern_type(cluster)
            
            # Get or create history for this pattern type
            if pattern_type not in self.pattern_history:
                self.pattern_history[pattern_type] = {
                    "pattern_type": pattern_type,
                    "total_occurrences": 0,
                    "predictions_correct": 0,
                    "predictions_wrong": 0,
                    "prediction_accuracy": None,
                    "average_resolution_time_minutes": None,
                    "last_seen_timestamp": None,
                    "last_outcome": None,
                    "second_to_last_outcome": None,
                    "current_confidence_modifier": 1.0,
                    "confidence_adjustment_log": []
                }
            
            history = self.pattern_history[pattern_type]
            
            # Calculate calibration
            calibration = self._apply_calibration_rules(
                pattern_id,
                pattern_type,
                cluster,
                history,
                current_time
            )
            
            calibrations.append(calibration)
            self.calibrations.append(calibration)
        
        # Save updated history
        self._save_pattern_history()
        
        return calibrations
    
    def _apply_calibration_rules(
        self,
        pattern_id: str,
        pattern_type: str,
        cluster: FailureCluster,
        history: Dict[str, Any],
        current_time: datetime
    ) -> Dict[str, Any]:
        """
        Apply calibration rules to determine confidence baseline
        """
        rules_applied = []
        rule_overrides = []
        
        # Assume pattern was just detected (time_since = 0 for new patterns)
        time_since_first_detected_minutes = 0
        
        base_modifier = history["current_confidence_modifier"]
        previous_modifier = base_modifier
        
        # RULE 1: FRESH PATTERN OVERRIDE
        if history["total_occurrences"] <= 2:
            base_modifier = 0.80
            rules_applied.append("RULE_1")
            rule_overrides.append("RULE_1 overrode RULE_2, RULE_3, RULE_4")
            reasoning = f"Rule 1 triggered. Total occurrences is {history['total_occurrences']} ‚Äî pattern type has insufficient history. Base modifier set to 0.80 per fresh pattern override."
            briefing = f"This pattern type ({pattern_type}) has been seen {history['total_occurrences']} times. Insufficient history to calibrate. Approach with standard caution."
        
        # RULE 2: STALENESS RESET
        elif history["last_seen_timestamp"]:
            last_seen = datetime.fromisoformat(history["last_seen_timestamp"])
            hours_since = (current_time - last_seen).total_seconds() / 3600
            
            if hours_since > 48:
                base_modifier = 1.0
                rules_applied.append("RULE_2")
                rule_overrides.append("RULE_2 overrode RULE_3, RULE_4")
                reasoning = f"Rule 2 triggered. Last seen {hours_since:.1f} hours ago (>{48}h threshold). History too old, conditions likely changed. Resetting to 1.0."
                briefing = f"Pattern type hasn't occurred in {hours_since:.0f} hours. Starting fresh ‚Äî old data may not be relevant."
            else:
                # RULE 3: PREDICTION ACCURACY MODIFIER
                base_modifier = self._apply_rule_3(base_modifier, history, rules_applied)
                
                # RULE 4: CONSECUTIVE MISS PENALTY
                base_modifier = self._apply_rule_4(base_modifier, history, rules_applied)
                
                reasoning = self._build_calibration_reasoning(history, rules_applied, base_modifier, previous_modifier)
                briefing = self._build_council_briefing(history, pattern_type)
        else:
            # First time seeing this pattern type
            base_modifier = 0.80
            rules_applied.append("RULE_1")
            rule_overrides.append("RULE_1 overrode RULE_2, RULE_3, RULE_4")
            reasoning = "Rule 1 triggered. Pattern type never seen before. Starting with 0.80 baseline."
            briefing = f"First occurrence of {pattern_type}. No track record to reference. Approach with standard caution."
        
        # Apply staleness decay
        staleness_decay = self._calculate_staleness_decay(time_since_first_detected_minutes)
        final_confidence_baseline = base_modifier * staleness_decay
        
        # Apply confidence floor
        confidence_floor_hit = False
        if final_confidence_baseline < 0.40:
            final_confidence_baseline = 0.40
            confidence_floor_hit = True
        
        # Determine if rescan needed
        needs_rescan = staleness_decay <= 0.70 or final_confidence_baseline <= 0.50
        
        return {
            "calibration_id": f"CAL-{pattern_id}-{int(current_time.timestamp())}",
            "pattern_id": pattern_id,
            "pattern_type": pattern_type,
            "rules_applied": rules_applied,
            "rule_overrides": rule_overrides,
            "base_modifier": round(base_modifier, 2),
            "staleness_decay": round(staleness_decay, 2),
            "final_confidence_baseline": round(final_confidence_baseline, 2),
            "confidence_floor_hit": confidence_floor_hit,
            "needs_rescan": needs_rescan,
            "reasoning": reasoning,
            "council_briefing": briefing,
            "adjustment_log_entry": {
                "timestamp": current_time.isoformat(),
                "pattern_type": pattern_type,
                "previous_modifier": round(previous_modifier, 2),
                "new_modifier": round(base_modifier, 2),
                "reason": " + ".join(rules_applied) if rules_applied else "No change"
            }
        }
    
    def _apply_rule_3(self, base_modifier: float, history: Dict[str, Any], rules_applied: List[str]) -> float:
        """Apply prediction accuracy modifier"""
        accuracy = history.get("prediction_accuracy")
        
        if accuracy is None or history["total_occurrences"] == 0:
            return base_modifier
        
        rules_applied.append("RULE_3")
        
        if accuracy >= 0.85:
            return base_modifier  # No change
        elif accuracy >= 0.70:
            return base_modifier * 0.92
        elif accuracy >= 0.50:
            return base_modifier * 0.80
        else:
            return base_modifier * 0.65
    
    def _apply_rule_4(self, base_modifier: float, history: Dict[str, Any], rules_applied: List[str]) -> float:
        """Apply consecutive miss penalty"""
        last = history.get("last_outcome")
        second_last = history.get("second_to_last_outcome")
        
        if last == "prediction_miss" and second_last == "prediction_miss":
            rules_applied.append("RULE_4")
            return base_modifier * 0.70
        elif last == "prediction_miss":
            rules_applied.append("RULE_4")
            return base_modifier * 0.88
        
        return base_modifier
    
    def _calculate_staleness_decay(self, minutes: int) -> float:
        """Calculate time-based decay factor"""
        if minutes <= 15:
            return 1.0
        elif minutes <= 30:
            return 0.95
        elif minutes <= 60:
            return 0.85
        elif minutes <= 120:
            return 0.70
        else:
            return 0.55
    
    def _build_calibration_reasoning(
        self,
        history: Dict[str, Any],
        rules_applied: List[str],
        base_modifier: float,
        previous_modifier: float
    ) -> str:
        """Build detailed reasoning for calibration decision"""
        parts = []
        
        if "RULE_3" in rules_applied:
            accuracy = history.get("prediction_accuracy", 0)
            parts.append(f"Rule 3: Prediction accuracy is {accuracy:.1%}. Applied modifier.")
        
        if "RULE_4" in rules_applied:
            if history.get("last_outcome") == "prediction_miss" and history.get("second_to_last_outcome") == "prediction_miss":
                parts.append("Rule 4: Two consecutive misses detected. Severe penalty applied.")
            else:
                parts.append("Rule 4: Single recent miss. Moderate penalty applied.")
        
        parts.append(f"Base modifier adjusted from {previous_modifier:.2f} to {base_modifier:.2f}.")
        
        return " ".join(parts)
    
    def _build_council_briefing(self, history: Dict[str, Any], pattern_type: str) -> str:
        """Build concise briefing for Council agents"""
        accuracy = history.get("prediction_accuracy", 0)
        occurrences = history.get("total_occurrences", 0)
        last_outcome = history.get("last_outcome", "")
        
        if last_outcome == "prediction_miss":
            return f"{pattern_type}: {accuracy:.0%} accuracy over {occurrences} occurrences. Recent miss detected ‚Äî be cautious."
        else:
            return f"{pattern_type}: {accuracy:.0%} accuracy over {occurrences} occurrences. Track record is {'strong' if accuracy > 0.8 else 'moderate'}."
    
    def _save_results(
        self,
        decisions: List[AgentDecision],
        metrics: PerformanceMetrics,
        start_time: float
    ) -> Dict[str, Any]:
        """
        Save decisions and metrics to JSON file
        """
        output = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "model_used": LLM_MODEL,
                "inference_time_seconds": round(time.time() - start_time, 2),
                "total_decisions": len(decisions)
            },
            "decisions": [d.model_dump() for d in decisions],
            "metrics": metrics.model_dump()
        }
        
        # Create data directory if needed
        Path("data").mkdir(exist_ok=True)
        
        # Save to file with UTF-8 encoding to preserve rupee symbol
        with open("data/decisions.json", 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        return output


def main():
    """
    Main execution function with error handling
    """
    print("=" * 60)
    print("SENTINEL COUNCIL AGENT - The Strategist")
    print("Multi-Persona LLM Reasoning Engine")
    print("=" * 60)
    print()
    
    try:
        agent = CouncilAgent()
        results = agent.analyze_failures()
        
        print("\n" + "=" * 60)
        print("ANALYSIS COMPLETE")
        print("=" * 60)
        print(f"\nüìä Summary:")
        print(f"   Decisions Made: {results['metadata']['total_decisions']}")
        print(f"   Inference Time: {results['metadata']['inference_time_seconds']}s")
        print(f"   Net Profit: ‚Çπ{results['metrics']['net_profit']:,.2f}")
        print(f"   Patterns Found: {results['metrics']['patterns_discovered']}")
        print(f"\nüíæ Results saved to: data/decisions.json")
        
    except FileNotFoundError as e:
        print(f"\n‚ùå Error: {e}")
        return 1
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrupted by user")
        return 130
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())
